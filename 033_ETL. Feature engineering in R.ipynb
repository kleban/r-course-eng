{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e723050a",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "---\n",
    "\n",
    "## Fundamentals of ETL: data extraction, transformation and loading\n",
    "\n",
    "\n",
    "Applied Mathematical Modeling in Banking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bd75d3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab820065",
   "metadata": {},
   "source": [
    "# Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb606911",
   "metadata": {},
   "source": [
    "1. What's Feature Engeniering?\n",
    "\n",
    "    1.2. Feature Scaling    \n",
    "        1.2.1. Normalization\n",
    "        1.2.2. Standardization        \n",
    "2. Feature Transformation\n",
    "3. Feature Construction\n",
    "\n",
    "    3.1. Binning\n",
    "    \n",
    "    3.2. Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28061ae",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0b45be",
   "metadata": {},
   "source": [
    "# 1. What's Feature Engineering?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b26f33",
   "metadata": {},
   "source": [
    "`Feature engineering` is the most important technique used in creating machine learning models. \n",
    "\n",
    "Feature Engineering is a basic term used to cover many operations that are performed on the variables(features) to fit them into the algorithm. **It helps in increasing the accuracy** of the model thereby enhances the results of the predictions. Feature Engineered machine learning models perform better on data than basic machine learning models. The following aspects of feature engineering are as follows [1]:\n",
    "\n",
    "1. `Feature Scaling`: It is done to get the features on the same scale( for eg. Euclidean distance).\n",
    "2. `Feature Transformation`: It is done to normalize the data(feature) by a function.\n",
    "3. `Feature Construction`: It is done to create new features based on original descriptors to improve the accuracy of the predictive model.\n",
    "4. `Feature Reduction` : It is done to improve the statistical distribution and accuracy of the predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81d7d1a",
   "metadata": {},
   "source": [
    "A `\"feature\"` in the context of predictive modeling is just another name for a `predictor variable`. Feature engineering is the general term for creating and manipulating predictors so that a good predictive model can be created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef5ee84",
   "metadata": {},
   "source": [
    "## 1.2. Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22e96ab",
   "metadata": {},
   "source": [
    "`Feature Scaling` refers to putting the values in the same range or same scale so that no variable is dominated by the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666a57a5",
   "metadata": {},
   "source": [
    "Most of the times, your dataset will contain features highly varying in magnitudes, units and range. But since, most of the machine learning algorithms use Euclidean distance between two data points in their computations, this is a problem.\n",
    "\n",
    "If left alone, these algorithms only take in the magnitude of features neglecting the units. The results would vary greatly between different units, 5kg and 5000gms. The features with high magnitudes will weigh in a lot more in the distance calculations than features with low magnitudes. To suppress this effect, we need to bring all features to the same level of magnitudes. This can be achieved by scaling.\n",
    "\n",
    "Here’s the curious thing about feature scaling – it improves (significantly) the performance of some machine learning algorithms and does not work at all for others.\n",
    "\n",
    "Also, what’s the difference between normalization and standardization? These are two of the most commonly used feature scaling techniques in machine learning but a level of ambiguity exists in their understanding. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74664ac",
   "metadata": {},
   "source": [
    "### 1.2.1. Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbd9ebd",
   "metadata": {},
   "source": [
    "Normalization is a scaling technique in which values are shifted and rescaled so that they end up ranging between 0 and 1. It is also known as Min-Max scaling.\n",
    "\n",
    "Here’s the formula for normalization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808e753d",
   "metadata": {},
   "source": [
    "<center>$X' = \\frac{X-X_{min}}{X_{max} - X_{min}}$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a960f4c",
   "metadata": {},
   "source": [
    "Here, $X_{max}$ and $X_{min}$ are the maximum and the minimum values of the feature respectively.\n",
    "\n",
    "When the value of $X$ is the minimum value in the column, the numerator will be $0$, and hence $X'$ is $0$.\n",
    "\n",
    "On the other hand, when the value of $X$ is the maximum value in the column, the numerator is equal to the denominator and thus the value of $X'$ is $1$.\n",
    "\n",
    "If the value of $X$ is between the minimum and the maximum value, then the value of $X'$ is between $0$ and $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2a8e79",
   "metadata": {},
   "source": [
    "### 1.2.2. Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83387702",
   "metadata": {},
   "source": [
    "`Standardization` is another scaling technique where the values are centered around the mean with a unit standard deviation. This means that the mean of the attribute becomes zero and the resultant distribution has a unit standard deviation.\n",
    "\n",
    "Here’s the formula for standardization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd295a2",
   "metadata": {},
   "source": [
    "<center>$X' = \\frac{X-\\mu}{\\sigma}$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dd71d2",
   "metadata": {},
   "source": [
    "Feature scaling: $\\mu$ is the mean of the feature values and Feature scaling: $\\sigma$ is the standard deviation of the feature values. Note that in this case, the values are not restricted to a particular range.\n",
    "\n",
    "Now, the big question in your mind must be when should we use normalization and when should we use standardization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784d02c2",
   "metadata": {},
   "source": [
    "Normalization vs. standardization is an eternal question among machine learning newcomers. Let me elaborate on the answer in this section.\n",
    "\n",
    "Normalization is good to use when you know that the distribution of your data does not follow a Gaussian distribution. This can be useful in algorithms that do not assume any distribution of the data like K-Nearest Neighbors and Neural Networks.\n",
    "\n",
    "Standardization, on the other hand, can be helpful in cases where the data follows a Gaussian distribution. However, this does not have to be necessarily true. Also, unlike normalization, standardization does not have a bounding range. So, even if you have outliers in your data, they will not be affected by standardization.\n",
    "However, at the end of the day, the choice of using normalization or standardization will depend on your problem and the machine learning algorithm you are using. There is no hard and fast rule to tell you when to normalize or standardize your data. You can always start by fitting your model to raw, normalized and standardized data and compare the performance for best results.\n",
    "\n",
    "It is a good practice to fit the scaler on the training data and then use it to transform the testing data. This would avoid any data leakage during the model testing process. Also, the scaling of target values is generally not required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a902c6",
   "metadata": {},
   "source": [
    "## 2. Feature Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893cc22c",
   "metadata": {},
   "source": [
    "`Feature transformation` involves manipulating a predictor variable in some way so as to improve its performance in the predictive model. A variety of considerations come into play when transforming models, including:\n",
    "\n",
    "- [x] The `flexibility` of machine learning and statistical models in dealing with different types of data. For example, some techniques require that the input data be in numeric format, whereas others can deal with other formats, such as categorical, text, or dates.\n",
    "- [x] `Ease of interpretation`. A predictive model where all the predictors are on the same scale (e.g., have a mean of 0 and a standard deviation of 1), can make interpretation easier.\n",
    "- [x] `Predictive accuracy`. Some transformations of variables can improve the accuracy of prediction (e.g., rather than including a numeric variable as a predictor, instead include both it and a second variable that is its square).\n",
    "- [x] `Theory`. For example, economic theory dictates that in many situations the natural logarithm of data representing prices and quantities should be used.\n",
    "- [x] `Computational error`. Many algorithms are written in such a way that \"large\" numbers cause them to give the wrong result, where \"large\" may not be so large (e.g., more than 10 or less than -10)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e8a78c",
   "metadata": {},
   "source": [
    "## 3. Feature Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ec030b",
   "metadata": {},
   "source": [
    "The **`feature Construction`** method helps in creating new features in the data thereby increasing model accuracy and overall predictions. It is of two types:\n",
    "\n",
    "- [x] `Binning`: Bins are created for continuous variables.\n",
    "- [x] `Encoding`: Numerical variables or features are formed from categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c29ea67",
   "metadata": {},
   "source": [
    "### 3.1. Binning\n",
    "\n",
    "`Binning` is done to create bins for continuous variables where they are converted to categorical variables. There are two types of binning: `Unsupervised` and `Supervised`.\n",
    "\n",
    "`Unsupervised Binning` involves Automatic and Manual binning. In Automatic Binning, bins are created without human interference and are created automatically.  In Manual Binning, bins are created with human interference and we specify where the bins to be created.\n",
    "\n",
    "`Supervised Binning` involves creating bins for the continuous variable while taking the target variable into the consideration also."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44613f5d",
   "metadata": {},
   "source": [
    "### 3.2. Encoding\n",
    "\n",
    "`Encoding` is the process in which numerical variables or features are created from `categorical` variables. It is a widely used method in the industry and in every model building process. It is of two types: `Label Encoding` and `One-hot Encoding`.\n",
    "\n",
    "`Label Encoding` involves assigning each label a unique integer or value based on alphabetical ordering. It is the most popular and widely used encoding.\n",
    "\n",
    "`One-hot Encoding` involves creating additional features or variables on the basis of unique values in categorical variables i.e. every unique value in the category will be added as a new feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed880d6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3b131b",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34612e4d",
   "metadata": {},
   "source": [
    "1. [Feature Engineering in R Programming](https://www.geeksforgeeks.org/feature-engineering-in-r-programming/) by @dhruv5819\n",
    "2. [What is Feature Engineering?](https://www.displayr.com/what-is-feature-engineering/) by Tim Bok\n",
    "3. [Feature Scaling-Why it is required?](https://medium.com/@rahul77349/feature-scaling-why-it-is-required-8a93df1af310) by Rahul Saini\n",
    "4. [Feature Scaling for Machine Learning: Understanding the Difference Between Normalization vs. Standardization](https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/) by \n",
    "ANIRUDDHA BHANDARI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
